# EIBSC_pAIllow_October2021

# Explainable AI for the end user
## Internship at dataroots February - May 2023

**Author: Marie Vandecavey**
**Coaches: Fisher Kuan & Chiel Mues**
**Company supervisors: Hans Tierens & Virginie Marelli**
**Promotor: Prof. dr. ir. Hendrik Blockeel**
**University: KU Leuven**

This project creates a dashboard meant to render an AI system more understandable for non-technical end-users.
The AI application selected here is Natural Language Processing (NLP) and more specifically Alpaca LoRA, the low rank adaptation of the pre-trained LLaMA model developed by Meta AI (https://ai.facebook.com/blog/large-language-model-llama-meta-ai/ and https://arxiv.org/abs/2302.13971). This pre-trained model was accessed through https://huggingface.co/decapoda-research/llama-7b-hf.\
Alpaca LoRA is presented by Eric J. Wang: https://github.com/tloen/alpaca-lora. It is an adapted version of Stanford Alpaca (https://github.com/tatsu-lab/stanford_alpaca and https://crfm.stanford.edu/2023/03/13/alpaca.html) but is less cumbersome to fine-tune. \
The fine-tuning for this project was done based on 
* Sam Witteveen: https://www.youtube.com/watch?v=LSoqyynKU9E
* https://www.mlexpert.io/machine-learning/tutorials/alpaca-and-llama-inference
* https://colab.research.google.com/drive/15VstUxU48CT3mRudFrj3FIv6Z4cIXnon?usp=sharing#scrollTo=FETBsmJVk8Tv

The main data are the ones presented by tatsu-lab, created based on some initial human-generated data and using the self-instruct project (https://github.com/yizhongw/self-instruct). Additionally, 2 improved versions of this dataset were also used to appreciate the impact of data quality: first a cleaned version of the dataset, and then one for which the instructions are the same, but the output was generated by GPT-4 instead of ChatGPT.
* The initial human-generated data: seed_tasks.jsonl (https://github.com/tatsu-lab/stanford_alpaca)
* The original dataset: alpaca_data.json (https://github.com/tatsu-lab/stanford_alpaca)
* The cleaned dataset: alpaca_data_cleaned.json (https://github.com/gururise/AlpacaDataCleaned)
* The GPT-4 dataset: alpaca_gpt4_data.json (https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)

Moreover, this project also incorporates an ethical analysis of the data, specifically focusing on gender biases to assess unfairness. The package used for this purpose is GenBit, part of the Responsible AI toolbox: https://github.com/microsoft/responsible-ai-toolbox-genbit


**Important notes:** 
* All the code was run in several tabs in Google Colab apart for the  3 notebooks that take care of fine-tuning the model that were run on a A100 GPU.
* The other notebooks that use the fine-tuned model require at least the free GPU of Google Colab.
* Not all datafiles can be found in this repository for reasons of storage. However, starting from the links provided above and the codes in this repository, they can all be reconstructed.
* Similarly, the model files cannot be found here.
* For datarootians, all the files from the internship, including the csv files with all the data used and the model files can be found in Google Drive under [https://drive.google.com/drive/folders/1YPliqOhMkwcW-Mxq5XHI2VpA9-Q30M9X?usp=drive_link](https://drive.google.com/drive/folders/1YPliqOhMkwcW-Mxq5XHI2VpA9-Q30M9X?usp=drive_link)
